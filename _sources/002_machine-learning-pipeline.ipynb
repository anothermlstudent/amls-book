{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ttmwk8ANXnn6"
   },
   "source": [
    "## What I will learn if study this notebook?\n",
    "You will learn how ML experts suggest:\n",
    "- to report the performance of your models,\n",
    "- to select the hyperparameters of your models.\n",
    "\n",
    "TL, DR\n",
    "- It doesn't make sense to quantify how good your algorithm is over all possible data-generating distributions, since **no free lunch theorem** proves that all algorithm have the same average performance, instead\n",
    "- you want to quantify (measure) how good is your algorithm at a group of data-generating distributions related to a specific problem,\n",
    "- in particular, you want to estimate how good the algorithm will be when it will process new data (data outside the one that was used for training)\n",
    "- To estimate that **out-of-sample performance** of an algorithm you can use **k-fold cross-validation**.\n",
    "- That out-of-sample performance, in practice, is reported along with a 95% **confidence interval** (you will need to estimate the score and the **standard error**).\n",
    "- Since most of the times, algorithms have **hyperparameters** there is a question on how to select them:\n",
    "- You can select the best hyperparameters by the **one-standard-error rule**.\n",
    "- Another option is to integrate the hyperparameter selection into the algorithm itself by using **cross-validation**. To estimate the out-of-sample performance of that resulting algorithm you can use cross-validation, effectively getting what is known as **nested cross-validation**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qa3rsZi-TniS"
   },
   "source": [
    "\n",
    "## How good is this algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcr2wbvTkK3F"
   },
   "source": [
    "It doesn't make sense to quantify how good your algorithm is over all possible data-generating distributions, since **no free lunch theorem** proves that all algorithm have the same average performance, instead you want to quantify (measure) how good is your algorithm at a group of data-generating distributions related to a specific problem, in particular, you want to estimate how good the algorithm will be when it will process new data (data outside the one that was used for training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vOpgY6Hd1Pn"
   },
   "source": [
    "\"The **no free lunch theorem** for machine learning {cite}`wolpert_free_1997` states that, averaged over all possible data-generating distributions, every classification algorithm has the same error rate when classifying previously unobserved points\" From {cite}`goodfellow_deep_2016`, Pag. 113"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcDVkbgOOf6a"
   },
   "source": [
    "## Can I say that my algorithm is the best?\n",
    "\n",
    "\"In machine learning experiments, it is common to say that algorithm A is better than algorithm B if the upper bound of the 95 percent of the condifence interval for the error of the algorithm A is less than the lower bound of the of the 95 percent of the condifence interval for the error of the algorithm B.\"\n",
    "From {cite}`goodfellow_deep_2016`, Pag. 125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xi_fExZBl6oi"
   },
   "source": [
    "TODO\n",
    "\n",
    "Add plot with an example to understand the statement visually.\n",
    "\n",
    "so how I can estimate the out of sample error and the confidence intervals (the dot and the bars)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xi_fExZBl6oi"
   },
   "source": [
    "We can use the following code:\n",
    "\n",
    "```python\n",
    "scores = cross_val_score(reg, X, y, cv=k,scoring='neg_mean_squared_error')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 1.96))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xi_fExZBl6oi"
   },
   "source": [
    "To understand the explanation we need two theoretical results regarding the statistic $\\bar{X}$. Which is an unbiased estimator of the mean $\\mu$.\n",
    "\n",
    "$\\hat{\\mu}$ score means\n",
    "$\\hat{SE}$\n",
    "\n",
    "95% Confidence Interval\n",
    "performance $\\hat{\\mu}$ +/- 1.96 $\\hat{SE}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oggR0Z-ojFnv"
   },
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be_43t0flmGw"
   },
   "source": [
    ">**cross-validation** can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility. The process of evaluating a modelâ€™s performance is known as **model assessment**, whereas the process of selecting the proper level of flexibility for a model is known as **model selection**.\" \n",
    ">\n",
    "> From {cite}`james_introduction_2014`, Pag. 175"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gs0LakjCjM3Q"
   },
   "source": [
    "There are severla types of **cross-validation** procedures:\n",
    "- k-fold cross-validation\n",
    "- leave-one-out cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7YnnvCSTs1f"
   },
   "source": [
    "Now that we have our cross-validation scores and the estimates of the standard errors, **how should we select the best hyperparameter for our model?**\n",
    "We can follow the **one-standard-error rule**:\n",
    "\n",
    ">...\"if we repeated cross-validation using a different set of cross-validation folds, then the precise model with the lowest estimated test error would surely change. In this setting, we can select a model using the one-standard-error rule. We first calculate the standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve.\"\n",
    ">\n",
    ">From {cite}`james_introduction_2014` Pag. 214\n",
    "\n",
    ">...\"One problem is that no unbiased estimators of the variance of such average error estimator exist {cite}`bengio_unbiased_2004`, but approximations are typically used.\"\n",
    ">\n",
    "> From {cite}`goodfellow_deep_2016` Pag. 119"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ob9ZhmymdoW"
   },
   "source": [
    "## Discussions\n",
    "This topic can be consufing and tricky to implement.\n",
    "\n",
    "Little mistake, they say 95% confidence interval but they use 2 (which is ~95.45%) instead they should use 1.96 (for 95%)\n",
    "https://github.com/scikit-learn/scikit-learn/issues/1940\n",
    "\n",
    "In the first implementation, someone got confused and divide by the number of sample\n",
    "https://github.com/scikit-learn/scikit-learn/issues/6059\n",
    "https://github.com/scikit-learn/scikit-learn/pull/6072\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{bibliography} ./references.bib\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2N0oQCvxs4s",
    "outputId": "71d99bbc-1408-42f7-a820-5ddad8e718df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -22862.90680958,  -36520.18051482, -124486.1325756 ,\n",
       "         -2205.68974692])"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data[:16]\n",
    "y = diabetes.target[:16]\n",
    "reg = LinearRegression()\n",
    "\n",
    "k = 4\n",
    "scores = cross_val_score(reg, X, y, cv=k,scoring='neg_mean_squared_error')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Jzgvn__xs4t",
    "outputId": "8578cdda-c663-4c35-9a13-416620566a76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -22862.90680958,  -36520.18051482, -124486.1325756 ,\n",
       "         -2205.68974692])"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "inds=list(range(len(X)))\n",
    "fold_length = int(len(inds)/k)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for i in range(0,len(inds),fold_length):\n",
    "    train_inds = np.delete(inds, inds[i:i+fold_length])\n",
    "    test_inds = inds[i:i+fold_length]\n",
    "\n",
    "    reg.fit(X[train_inds],y[train_inds])\n",
    "\n",
    "    y_pred = reg.predict(X[test_inds])\n",
    "    y_true = y[test_inds]\n",
    "\n",
    "    scores.append(-mean_squared_error(y_true, y_pred))\n",
    "\n",
    "scores = np.array(scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FFVZCda8IzJW",
    "outputId": "6f3db2c9-ddbc-4ac5-e65a-1d3a540e642a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -9.5593196 , -37.69944289, -12.77935441,  -2.11896031])"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data[:16]\n",
    "y = diabetes.target[:16]\n",
    "reg = LinearRegression()\n",
    "\n",
    "k = 4\n",
    "scores = cross_val_score(reg, X, y, cv=k)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZSU3Z3fLnUYp",
    "outputId": "c6684733-7c1c-41d6-e281-220abbd199bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -9.5593196 , -37.69944289, -12.77935441,  -2.11896031])"
      ]
     },
     "execution_count": 115,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "inds=list(range(len(X)))\n",
    "fold_length = np.floor(len(inds)/k).astype('int')\n",
    "\n",
    "scores = []\n",
    "\n",
    "for i in range(0,len(inds),fold_length):\n",
    "    train_inds = np.delete(inds, inds[i:i+fold_length])\n",
    "    test_inds = inds[i:i+fold_length]\n",
    "\n",
    "    reg.fit(X[train_inds],y[train_inds])\n",
    "\n",
    "    y_pred = reg.predict(X[test_inds])\n",
    "    y_true = y[test_inds]\n",
    "\n",
    "    scores.append(r2_score(y_true, y_pred))\n",
    "\n",
    "scores = np.array(scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8YanPtHxQek"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Machine Learning Pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
