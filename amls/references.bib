---
---

@book{murphy_machine_2012,
   author = {Murphy, Kevin P.},
   title = {Machine Learning: A Probabilistic Perspective},
   publisher = {The MIT Press},
   abstract = {Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package--PMTK (probabilistic modeling toolkit)--that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students. },
   pages = {1104},
   ISBN = {978-0262018029},
   year = {2012},
   type = {Book}
}

@book{rasmussen_gaussian_2005,
author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
title = {Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)},
year = {2005},
isbn = {026218253X},
publisher = {The MIT Press}
}


@book{casella_statistical_2001,
  author = {Casella, George and Berger, Roger},
  isbn = {0534243126},
  month = {June},
  publisher = {{Duxbury Resource Center}},
  timestamp = {2009-10-28T04:42:57.000+0100},
  title = {Statistical Inference},
  year = 2001
}

@book{bishop_pattern_2007,
   author = {Bishop, Christopher},
   title = {Pattern Recognition and Machine Learning},
   publisher = {Springer},
   abstract = {This is the first textbook on pattern recognition to present the Bayesian viewpoint. The book presents approximate inference algorithms that permit fast approximate answers in situations where exact answers are not feasible. It uses graphical models to describe probability distributions when no other books apply graphical models to machine learning. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory.},
   ISBN = {978-0387310732},
   year = {2007},
   type = {Book}
}

@book{james_introduction_2014,
 author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
 title = {An Introduction to Statistical Learning: With Applications in R},
 year = {2014},
 isbn = {1461471370, 9781461471370},
 publisher = {Springer Publishing Company, Incorporated},
} 


@book{hastie_elements_2013,
   author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
   title = {The Elements of Statistical Learning: Data mining, inference and prediction},
   publisher = {Springer-Verlag},
   series = {Springer Series in Statistics},
   pages = {745},
   ISBN = {978-0387848570},
   year = {2013},
   type = {Book}
}

@book{goodfellow_deep_2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    year={2016}
}

@book{geron_hands_2017,
author = {Géron, Aurélien},
title = {Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems},
year = {2017},
isbn = {1491962291},
publisher = {O'Reilly Media, Inc.},
edition = {1st},
abstract = {Graphics in this book are printed in black and white. Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how. By using concrete examples, minimal theory, and two production-ready Python frameworksscikit-learn and Tensor Flowauthor Aurlien Gron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. Youll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what youve learned, all you need is programming experience to get started. Explore the machine learning landscape, particularly neural nets Use scikit-learn to track an example machine-learning project end-to-end Explore several training models, including support vector machines, decision trees, random forests, and ensemble methods Use the Tensor Flow library to build and train neural nets Dive into neural net architectures, including convolutional nets, recurrent nets, and deep reinforcement learning Learn techniques for training and scaling deep neural nets Apply practical code examples without acquiring excessive machine learning theory or algorithm details}
}

@article{bengio_unbiased_2004,
author = {Bengio, Yoshua and Grandvalet, Yves},
title = {No Unbiased Estimator of the Variance of K-Fold Cross-Validation},
year = {2004},
issue_date = {12/1/2004},
publisher = {JMLR.org},
volume = {5},
issn = {1532-4435},
abstract = {Most machine learning researchers perform quantitative experiments to estimate generalization error and compare the performance of different algorithms (in particular, their proposed algorithm). In order to be able to draw statistically convincing conclusions, it is important to estimate the uncertainty of such estimates. This paper studies the very commonly used K-fold cross-validation estimator of generalization performance. The main theorem shows that there exists no universal (valid under all distributions) unbiased estimator of the variance of K-fold cross-validation. The analysis that accompanies this result is based on the eigen-decomposition of the covariance matrix of errors, which has only three different eigenvalues corresponding to three degrees of freedom of the matrix and three components of the total variance. This analysis helps to better understand the nature of the problem and how it can make naive estimators (that don't take into account the error correlations due to the overlap between training and test sets) grossly underestimate variance. This is confirmed by numerical experiments in which the three components of the variance are compared when the difficulty of the learning problem and the number of folds are varied.},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {1089–1105},
numpages = {17}
}

@article{wolpert_free_1997,
author = {Wolpert, D. H. and Macready, W. G.},
title = {No Free Lunch Theorems for Optimization},
year = {1997},
issue_date = {April 1997},
publisher = {IEEE Press},
volume = {1},
number = {1},
issn = {1089-778X},
url = {https://doi.org/10.1109/4235.585893},
doi = {10.1109/4235.585893},
abstract = {A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of “no free lunch” (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori “head-to-head” minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems' enforcing of a type of uniformity over all algorithms},
journal = {Trans. Evol. Comp},
month = apr,
pages = {67–82},
numpages = {16}
}

@book{parmigiani_decision_2009,
  title={Decision theory: Principles and approaches},
  author={Parmigiani, Giovanni and Inoue, Lurdes},
  volume={812},
  year={2009},
  publisher={John Wiley \& Sons}
}

@Inbook{berger_statistical_1989,
author="Berger, James O.",
editor="Eatwell, John
and Milgate, Murray
and Newman, Peter",
title="Statistical Decision Theory",
bookTitle="Game Theory",
year="1989",
publisher="Palgrave Macmillan UK",
address="London",
pages="217--224",
abstract="Decision theory is the science of making optimal decisions in the face of uncertainty. Statistical decision theory is concerned with the making of decisions when in the presence of statistical knowledge (data) which sheds light on some of the uncertainties involved in the decision problem. The generality of these definitions is such that decision theory (dropping the qualifier `statistical' for convenience) formally encompasses an enormous range of problems and disciplines. Any attempt at a general review of decision theory is thus doomed; all that can be done is to present a description of some of the underlying ideas.",
isbn="978-1-349-20181-5",
doi="10.1007/978-1-349-20181-5_26",
url="https://doi.org/10.1007/978-1-349-20181-5_26"
}

@book{render_quantitative_2017,
  title={Quantitative Analysis for Management},
  author={Render, B. and Stair, R.M. and Hanna, M.E. and Hale, T.S.},
  isbn={9780134543451},
  url={https://books.google.fr/books?id=SPT0DQAAQBAJ},
  year={2017},
  publisher={Pearson Education}
}
